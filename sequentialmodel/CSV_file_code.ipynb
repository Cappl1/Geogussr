{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea9af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "df = pd.read_csv(dir+\"\\coordinates.csv\", delimiter=',', skiprows=0, low_memory=False)\n",
    "\n",
    "\n",
    "# We want a geohash precsion of 3 so that we get approximately 32768 cells, which will represent our classes.\n",
    "df['geohash']=df.apply(lambda coords: phg.encode(coords.latitude, coords.longitude, precision=3), axis=1)\n",
    "\n",
    "\n",
    "def geohash_to_decimal(geohash):\n",
    "    \n",
    "    # get base_32 alphabet\n",
    "    base_32 = '0123456789bcdefghjkmnpqrstuvwxyz';\n",
    "    geohash = geohash.lower()\n",
    "    \n",
    "    # sanity check\n",
    "    if not all(c in base_32 for c in geohash):\n",
    "        raise ValueError('Invalid geohash')\n",
    "        \n",
    "    return sum([32**idx * base_32.index(char) for idx, char in enumerate(geohash[::-1])])\n",
    "\n",
    "# change geohash to decimal number\n",
    "df['geohash_decimal']=df.apply(lambda x: geohash_to_decimal(x[\"geohash\"]) ,axis=1)\n",
    "\n",
    "# get all hashes that contain samples\n",
    "geohashes_with_samples = df[\"geohash_decimal\"].unique()\n",
    "print(\"Number of geohashes with samples\", len(geohashes_with_samples))\n",
    "\n",
    "# map the decimal_geohashes with samples to cluster_numbers\n",
    "geohash_map = { geo: i for i, geo in enumerate(geohashes_with_samples)}\n",
    "\n",
    "df[\"geo_code\"] = df.apply(lambda geohash: geohash_map[geohash[\"geohash_decimal\"]], axis=1)\n",
    "\n",
    "df[[\"filename\", \"latitude\",\"longitude\", \"geohash_decimal\", \"geo_code\"]].to_csv(dir+\"\\coordinates2.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add geohash center coordinates to csv\n",
    "\n",
    "def geohash_center(geohash_name):\n",
    "\n",
    "    # Decode the geohash to get the center coordinates and errors.\n",
    "    latitude, longitude, latitude_error, longitude_error = phg.decode_exactly(geohash_name)\n",
    "    return [latitude, longitude]\n",
    "\n",
    "df[\"geo_lat\"], df[\"geo_lon\"] = df.apply(lambda x: geohash_center(x[\"geohash\"])[0] ,axis=1), df.apply(lambda x: geohash_center(x[\"geohash\"])[1] ,axis=1)\n",
    "\n",
    "# drop the duplicates\n",
    "df = df.drop(columns=[\"filename\", \"latitude\", \"longitude\",\"geohash_decimal\",\"geohash\"] )\n",
    "df = df.drop_duplicates()\n",
    "array = df.to_numpy()\n",
    "array = np.array(array, dtype=np.float64)\n",
    "tensor = torch.tensor(array)\n",
    "torch.save(tensor, dir+'\\\\tensor.pt')\n",
    "# Save the DataFrame to a CSV file.\n",
    "df[[\"geo_code\", \"geo_lat\", \"geo_lon\"]].to_csv(dir+\"\\coords_center.csv\", index=False)\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(dir+\"\\\\all_new_continents_merged.csv\", delimiter=',', skiprows=0, low_memory=False)\n",
    "df2 = pd.read_csv(dir+\"\\coordinates2.csv\", delimiter=',', skiprows=0, low_memory=False)\n",
    "\n",
    "df1_country = df1[['continent']]\n",
    "\n",
    "contis = df1_country['continent'].unique()\n",
    "\n",
    "\n",
    "conti_map = { conti: i for i, conti in enumerate(contis)}\n",
    "print(contis)\n",
    "df1_country[\"conti_code\"] = df1_country.apply(lambda continent: conti_map[continent[\"continent\"]], axis=1)\n",
    "\n",
    "df3 = pd.concat([df2,df1_country], axis=1)\n",
    "df3.to_csv(dir+\"\\coordinates3.csv\", index=False)\"\"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
